{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled15.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOALWv+rwDzrMXw0joI7RWH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atlmapguy/netflix/blob/main/colfil.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrtN2d3bXVW7",
        "outputId": "26096a05-5a4f-44d2-bef8-48d653aede73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-27 14:16:24--  https://datasets.towardsai.net/combined_data_4.txt\n",
            "Resolving datasets.towardsai.net (datasets.towardsai.net)... 104.26.1.89, 172.67.74.231, 104.26.0.89, ...\n",
            "Connecting to datasets.towardsai.net (datasets.towardsai.net)|104.26.1.89|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://zenodo.org/record/4556134/files/combined_data_4.txt?download=1 [following]\n",
            "--2022-04-27 14:16:25--  https://zenodo.org/record/4556134/files/combined_data_4.txt?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 137.138.76.77\n",
            "Connecting to zenodo.org (zenodo.org)|137.138.76.77|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 552537802 (527M) [text/plain]\n",
            "Saving to: ‘combined_data_4.txt’\n",
            "\n",
            "combined_data_4.txt  10%[=>                  ]  56.85M  77.2KB/s    eta 57m 55s"
          ]
        }
      ],
      "source": [
        "# Recommendation System Tutorial - Netflix\n",
        "# URL: https://towardsai.net/recommendation-system-tutorial\n",
        "\n",
        "# Download datasets\n",
        "!wget https://datasets.towardsai.net/combined_data_4.txt\n",
        "!wget https://raw.githubusercontent.com/towardsai/tutorials/master/recommendation_system_tutorial/movie_titles.csv\n",
        "!wget https://raw.githubusercontent.com/towardsai/tutorials/master/recommendation_system_tutorial/new_features.csv\n",
        "\n",
        "!pip install scikit-surprise\n",
        "\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import os\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import sparse\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import xgboost as xgb\n",
        "from surprise import Reader, Dataset\n",
        "from surprise import BaselineOnly\n",
        "from surprise import KNNBaseline\n",
        "from surprise import SVD\n",
        "from surprise import SVDpp\n",
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "def load_data():\n",
        "    netflix_csv_file = open(\"netflix_rating.csv\", mode = \"w\")\n",
        "    rating_files = ['combined_data_4.txt']\n",
        "    for file in rating_files:\n",
        "        with open(file) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line.endswith(\":\"):\n",
        "                    movie_id = line.replace(\":\", \"\")\n",
        "                else:\n",
        "                    row_data = []\n",
        "                    row_data = [item for item in line.split(\",\")]\n",
        "                    row_data.insert(0, movie_id)\n",
        "                    netflix_csv_file.write(\",\".join(row_data))\n",
        "                    netflix_csv_file.write('\\n')\n",
        "\n",
        "    netflix_csv_file.close()\n",
        "    df = pd.read_csv('netflix_rating.csv', sep=\",\", names = [\"movie_id\",\"customer_id\", \"rating\", \"date\"])\n",
        "    return df\n",
        "\n",
        "netflix_rating_df = load_data()\n",
        "netflix_rating_df\n",
        "netflix_rating_df.head()\n",
        "\n",
        "netflix_rating_df.duplicated([\"movie_id\",\"customer_id\", \"rating\", \"date\"]).sum()\n",
        "\n",
        "split_value = int(len(netflix_rating_df) * 0.80)\n",
        "train_data = netflix_rating_df[:split_value]\n",
        "test_data = netflix_rating_df[split_value:]\n",
        "\n",
        "plt.figure(figsize = (12, 8))\n",
        "ax = sns.countplot(x=\"rating\", data=train_data)\n",
        "\n",
        "ax.set_yticklabels([num for num in ax.get_yticks()])\n",
        "\n",
        "plt.tick_params(labelsize = 15)\n",
        "plt.title(\"Count Ratings in train data\", fontsize = 20)\n",
        "plt.xlabel(\"Ratings\", fontsize = 20)\n",
        "plt.ylabel(\"Number of Ratings\", fontsize = 20)\n",
        "plt.show()\n",
        "\n",
        "def get_user_item_sparse_matrix(df):\n",
        "    sparse_data = sparse.csr_matrix((df.rating, (df.customer_id, df.movie_id)))\n",
        "    return sparse_data\n",
        "\n",
        "train_sparse_data = get_user_item_sparse_matrix(train_data)\n",
        "\n",
        "test_sparse_data = get_user_item_sparse_matrix(test_data)\n",
        "\n",
        "global_average_rating = train_sparse_data.sum()/train_sparse_data.count_nonzero()\n",
        "print(\"Global Average Rating: {}\".format(global_average_rating))\n",
        "\n",
        "def get_average_rating(sparse_matrix, is_user):\n",
        "    ax = 1 if is_user else 0\n",
        "    sum_of_ratings = sparse_matrix.sum(axis = ax).A1\n",
        "    no_of_ratings = (sparse_matrix != 0).sum(axis = ax).A1\n",
        "    rows, cols = sparse_matrix.shape\n",
        "    average_ratings = {i: sum_of_ratings[i]/no_of_ratings[i] for i in range(rows if is_user else cols) if no_of_ratings[i] != 0}\n",
        "    return average_ratings\n",
        "\n",
        "average_rating_user = get_average_rating(train_sparse_data, True)\n",
        "\n",
        "avg_rating_movie = get_average_rating(train_sparse_data, False)\n",
        "\n",
        "total_users = len(np.unique(netflix_rating_df[\"customer_id\"]))\n",
        "train_users = len(average_rating_user)\n",
        "uncommonUsers = total_users - train_users\n",
        "\n",
        "print(\"Total no. of Users = {}\".format(total_users))\n",
        "print(\"No. of Users in train data= {}\".format(train_users))\n",
        "print(\"No. of Users not present in train data = {}({}%)\".format(uncommonUsers, np.round((uncommonUsers/total_users)*100), 2))\n",
        "\n",
        "total_movies = len(np.unique(netflix_rating_df[\"movie_id\"]))\n",
        "train_movies = len(avg_rating_movie)\n",
        "uncommonMovies = total_movies - train_movies\n",
        "\n",
        "print(\"Total no. of Movies = {}\".format(total_movies))\n",
        "print(\"No. of Movies in train data= {}\".format(train_movies))\n",
        "print(\"No. of Movies not present in train data = {}({}%)\".format(uncommonMovies, np.round((uncommonMovies/total_movies)*100), 2))\n",
        "\n",
        "def compute_user_similarity(sparse_matrix, limit=100):\n",
        "    row_index, col_index = sparse_matrix.nonzero()\n",
        "    rows = np.unique(row_index)\n",
        "    similar_arr = np.zeros(61700).reshape(617,100)\n",
        "\n",
        "    for row in rows[:limit]:\n",
        "        sim = cosine_similarity(sparse_matrix.getrow(row), train_sparse_data).ravel()\n",
        "        similar_indices = sim.argsort()[-limit:]\n",
        "        similar = sim[similar_indices]\n",
        "        similar_arr[row] = similar\n",
        "\n",
        "    return similar_arr\n",
        "\n",
        "similar_user_matrix = compute_user_similarity(train_sparse_data, 100)\n",
        "\n",
        "similar_user_matrix[0]\n",
        "\n",
        "movie_titles_df = pd.read_csv(\"movie_titles.csv\",sep = \",\",\n",
        "                              header = None, names=['movie_id', 'year_of_release', 'movie_title'],\n",
        "                              index_col = \"movie_id\", encoding = \"iso8859_2\")\n",
        "movie_titles_df.head()\n",
        "\n",
        "def compute_movie_similarity_count(sparse_matrix, movie_titles_df, movie_id):\n",
        "    similarity = cosine_similarity(sparse_matrix.T, dense_output = False)\n",
        "    no_of_similar_movies = movie_titles_df.loc[movie_id][1], similarity[movie_id].count_nonzero()\n",
        "    return no_of_similar_movies\n",
        "\n",
        "similar_movies = compute_movie_similarity_count(train_sparse_data, movie_titles_df, 1775)\n",
        "print(\"Similar Movies = {}\".format(similar_movies))\n",
        "\n",
        "def get_sample_sparse_matrix(sparse_matrix, no_of_users, no_of_movies):\n",
        "    users, movies, ratings = sparse.find(sparse_matrix)\n",
        "    uniq_users = np.unique(users)\n",
        "    uniq_movies = np.unique(movies)\n",
        "    np.random.seed(15)\n",
        "    user = np.random.choice(uniq_users, no_of_users, replace = False)\n",
        "    movie = np.random.choice(uniq_movies, no_of_movies, replace = True)\n",
        "    mask = np.logical_and(np.isin(users, user), np.isin(movies, movie))\n",
        "    sparse_matrix = sparse.csr_matrix((ratings[mask], (users[mask], movies[mask])),\n",
        "                                                     shape = (max(user)+1, max(movie)+1))\n",
        "    return sparse_matrix\n",
        "\n",
        "train_sample_sparse_matrix = get_sample_sparse_matrix(train_sparse_data, 400, 40)\n",
        "\n",
        "test_sparse_matrix_matrix = get_sample_sparse_matrix(test_sparse_data, 200, 20)\n",
        "\n",
        "def create_new_similar_features(sample_sparse_matrix):\n",
        "    global_avg_rating = get_average_rating(sample_sparse_matrix, False)\n",
        "    global_avg_users = get_average_rating(sample_sparse_matrix, True)\n",
        "    global_avg_movies = get_average_rating(sample_sparse_matrix, False)\n",
        "    sample_train_users, sample_train_movies, sample_train_ratings = sparse.find(sample_sparse_matrix)\n",
        "    new_features_csv_file = open(\"new_features.csv\", mode = \"w\")\n",
        "\n",
        "    for user, movie, rating in zip(sample_train_users, sample_train_movies, sample_train_ratings):\n",
        "        similar_arr = list()\n",
        "        similar_arr.append(user)\n",
        "        similar_arr.append(movie)\n",
        "        similar_arr.append(sample_sparse_matrix.sum()/sample_sparse_matrix.count_nonzero())\n",
        "\n",
        "        similar_users = cosine_similarity(sample_sparse_matrix[user], sample_sparse_matrix).ravel()\n",
        "        indices = np.argsort(-similar_users)[1:]\n",
        "        ratings = sample_sparse_matrix[indices, movie].toarray().ravel()\n",
        "        top_similar_user_ratings = list(ratings[ratings != 0][:5])\n",
        "        top_similar_user_ratings.extend([global_avg_rating[movie]] * (5 - len(ratings)))\n",
        "        similar_arr.extend(top_similar_user_ratings)\n",
        "\n",
        "        similar_movies = cosine_similarity(sample_sparse_matrix[:,movie].T, sample_sparse_matrix.T).ravel()\n",
        "        similar_movies_indices = np.argsort(-similar_movies)[1:]\n",
        "        similar_movies_ratings = sample_sparse_matrix[user, similar_movies_indices].toarray().ravel()\n",
        "        top_similar_movie_ratings = list(similar_movies_ratings[similar_movies_ratings != 0][:5])\n",
        "        top_similar_movie_ratings.extend([global_avg_users[user]] * (5-len(top_similar_movie_ratings)))\n",
        "        similar_arr.extend(top_similar_movie_ratings)\n",
        "\n",
        "        similar_arr.append(global_avg_users[user])\n",
        "        similar_arr.append(global_avg_movies[movie])\n",
        "        similar_arr.append(rating)\n",
        "\n",
        "        new_features_csv_file.write(\",\".join(map(str, similar_arr)))\n",
        "        new_features_csv_file.write(\"\\n\")\n",
        "\n",
        "    new_features_csv_file.close()\n",
        "    new_features_df = pd.read_csv('new_features.csv', names = [\"user_id\", \"movie_id\", \"gloabl_average\", \"similar_user_rating1\",\n",
        "                                                               \"similar_user_rating2\", \"similar_user_rating3\",\n",
        "                                                               \"similar_user_rating4\", \"similar_user_rating5\",\n",
        "                                                               \"similar_movie_rating1\", \"similar_movie_rating2\",\n",
        "                                                               \"similar_movie_rating3\", \"similar_movie_rating4\",\n",
        "                                                               \"similar_movie_rating5\", \"user_average\",\n",
        "                                                               \"movie_average\", \"rating\"])\n",
        "    return new_features_df\n",
        "\n",
        "train_new_similar_features = create_new_similar_features(train_sample_sparse_matrix)\n",
        "\n",
        "train_new_similar_features = train_new_similar_features.fillna(0)\n",
        "train_new_similar_features.head()\n",
        "\n",
        "test_new_similar_features = create_new_similar_features(test_sparse_matrix_matrix)\n",
        "\n",
        "test_new_similar_features = test_new_similar_features.fillna(0)\n",
        "test_new_similar_features.head()\n",
        "\n",
        "x_train = train_new_similar_features.drop([\"user_id\", \"movie_id\", \"rating\"], axis = 1)\n",
        "\n",
        "x_test = test_new_similar_features.drop([\"user_id\", \"movie_id\", \"rating\"], axis = 1)\n",
        "\n",
        "y_train = train_new_similar_features[\"rating\"]\n",
        "\n",
        "y_test = test_new_similar_features[\"rating\"]\n",
        "\n",
        "def error_metrics(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    return rmse\n",
        "\n",
        "clf = xgb.XGBRegressor(n_estimators = 100, silent = False, n_jobs  = 10)\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "y_pred_test = clf.predict(x_test)\n",
        "\n",
        "rmse_test = error_metrics(y_test, y_pred_test)\n",
        "print(\"RMSE = {}\".format(rmse_test))\n",
        "\n",
        "def plot_importance(model, clf):\n",
        "    fig = plt.figure(figsize = (8, 6))\n",
        "    ax = fig.add_axes([0,0,1,1])\n",
        "    model.plot_importance(clf, ax = ax, height = 0.3)\n",
        "    plt.xlabel(\"F Score\", fontsize = 20)\n",
        "    plt.ylabel(\"Features\", fontsize = 20)\n",
        "    plt.title(\"Feature Importance\", fontsize = 20)\n",
        "    plt.tick_params(labelsize = 15)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_importance(xgb, clf):\n",
        "    fig = plt.figure(figsize = (8, 6))\n",
        "    ax = fig.add_axes([0,0,1,1])\n",
        "    model.plot_importance(clf, ax = ax, height = 0.3)\n",
        "    plt.xlabel(\"F Score\", fontsize = 20)\n",
        "    plt.ylabel(\"Features\", fontsize = 20)\n",
        "    plt.title(\"Feature Importance\", fontsize = 20)\n",
        "    plt.tick_params(labelsize = 15)\n",
        "\n",
        "    plt.show()"
      ]
    }
  ]
}